---
title: "读书笔记-推荐系统相关知识"
date: 2023-02-01
toc: true
categories:
  - reco
classes: wide
words_per_minute: 10
---

# 1. 推荐系统简介

推荐系统需要众多模块合作：
- 日志系统去收集用户交互数据，落原始数据
- 数据加工：大数据系统、流式计算系统加工数据
- 模型更新：在线学习，系统及时去更新模型
- 监控系统：有监控体系去定期反馈模型是否运行正常，能够自动报警
- AB系统：有完善的模型、算法、策略评估体系，能够逐步优化推荐系统

## 1.1 各环节

| 环节 | 量级 | 角色                                                                                                                                                                       |
|------|------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 召回 | 百万 | 在短时间内，从物料库内召回接近用户兴趣的物料（剔除离谱的物料）。 由于速度要求，以及量级巨大，所以精度不做太高要求，以多路召回来弥补。 主要依赖“离线缓存，在线索引”的操作。 |
| 粗排 | 万   | 处于召回和精排之间，在数据量、模型结构复杂程度，相对平衡。                                                                                                                 |
| 精排 | 千   | 由于量级较小，所以模型结构可以较复杂，对精度要求较高。 在推荐链路靠后，能够较大影响最终结果。                                                                              |
| 重排 | 几十 | 对于精排结果有可能集聚的情况，把相似内容进行打散，减少用户审美疲劳。                                                                                                       |

## 1.2 数据架构：lambda架构

- 实时数据流
  - 在线层：利用 storm、flink等流式计算框架，用户行为不等数据落地，就直接分析计算，计算结果缓存到redis
- 历史日志
  - 离线层：小时级别的定时任务，去回溯过去较长周期的用户行为日志。hadoop、spark
  - 近线层：HDFS读写效率低的介质，所以结果导入redis这种键值型数据库，加速访问

# 2. 特征工程

比如DIN模型能够从用户行为序列挖掘出用户的短期兴趣，SIM模型能够挖掘长期兴趣。但不意味着不需要对用户的历史行为做任何特征工程。因为一些复杂模型，耗时会和候选集的规模成正比，不好用在粗排或者召回里。

特征工程技巧，能够将计算压力从线上转移到线下，离线挖掘出用户的长短期兴趣，提供到召回、粗排里进行在线训练和预测使用。

## 2.1 物料画像

物料的属性：最基础、直接的信息。比如视频的作者、作者等级、粉丝量、标题和简介、上传时间、清晰度。比如电商里，商品的介绍、图片、视频、上传时间、商铺（等级、关注量、物品量）等特征。

物料唯一标识（item ID）也是极为重要的，模型记忆即可。作为类别特征，是高维、稀疏的。

- 物料的类别和标签：内容理解算法，去做物料的分类以及置信度
- 物料基于内容emb：emb表征物料的内容，增加类别标签以外的扩展性
- 物料的动态画像：**后验**统计数据，反应物料的受欢迎程度
  - 比如：不同时间粒度，不同类型的统计（CTR、CVR、平均播放时长、评论等）
  - 本质：是一个后验指标，只能说明我们推荐的效果好，但不意味着给当前这个用户推送就是好的。作为特征是有偏的。如果参与到精排，所谓的"幸存者偏差"还不会那么严重。
  - 马太效应问题：将后验数据作为特征，不利于新物料的冷启动。
- 用户给物料的标签：比如用户的消费标签的比重，来调整当前的物料。比如，球星的八卦新闻，我们初始打标为"体育"，但经常消费"娱乐"的用户来消费它比重较大，说明应该是物料应该是偏"娱乐"

## 2.2 用户画像

- 用户静态画像
  - 对于没什么行为的新用户不友好
    - 新老用户共用一个模型，老用户贡献的样本多，主导了训练模型，不容易学习到新用户相对友好的特征
  - UserID特征
    - 上亿用户情况下，采用 parameter server 分布式支持
    - 提供了最细粒度的个性化信息
- 用户动态画像
  - 通过用户一段时间里交互过的物料 Item ID 按时间序列进行集合，在放到模型里去提取用户的兴趣
  - 和物料相关的模型
    - 候选物料：attention pooling
    - 无候选物料：普通的 pooling
  - 缺点：如果将提取兴趣和CTR模型结合在一起，都必须在线上完成，特别是复杂模型DIN等，耗时会跟候选集规模成正比。但实际上我们是希望从历史序列里去做长行为序列里的用户兴趣。
    - 改进：将提取用户兴趣和CTR建模解耦。通过离线去计算用户兴趣，存储后用user id 去查询相应指标，耗时就可以减少。

## 2.3 交叉特征

- 笛卡尔积交叉：两个field内的feature进行两两组合，形成一个新的field
- 内积交叉：比如双塔模型里，用户和物料的交互

## 2.4 偏差特征

背景：我们用某个行为来衡量用户是否喜欢物料，比如点击代表喜欢。但有可能用户点击了未必喜欢，喜欢的未必去点击。**候选物料之间的评估环境，并非绝对公平的。** 最常见的就是位置偏差。另外，Youtube还发现视频生命周期带来的偏差（比如，视频长传久了，后验的消费指标，比如点击率、人均观看时长、总观看时长、总播放量）都会比较高，导致马太效应。

如何解决：
- 更严格的定义正负样本：限制在 点击物料上方的未点击物料，才作为负样本。防止曝光后，喜欢但是未点击的物料被错误地视为负样本。
- 模型去定义：
  - 将位置偏差作为特征加到模型里，在推理时都填为零（因为模型此刻并不知道视频排序，所以都视为第一个；同时，不应该让该特征影响了DNN模块的预测排序）
  - 训练时，位置偏差特征作为**伪特征值**，应该单独加入到最后的logit值里，而不是加到DNN，让DNN里的其他特征跟它交互。

<figure>
  <img src="{{ '/assets/images/reco-shitaxi-1.png' | relative_url }}" alt="xgboost"  class="center" style="max-height:600px; max-width:800px">
</figure>

## 2.5 数值特征处理

- 处理缺失值
- 标准化
- 数据平滑和消偏
- 分桶离散化

## 2.6 类别特征

**类别特征是推荐算法的一等公民**

线上工程偏爱于类别特征，因为类别特征非常稀疏，可以实现非零存储，减少线上开销。以逻辑回归 LR 为例， $$ logit = b + sum(w_{j} * x) $$ 如果是类别型特征（0/1），则相加即可。如果是数值型特征，则需要很多乘法运算。

- 单个类别表达能力可能较弱：使用emb扩展其代表的语义
- 多特征交叉：将多个类别型特征进行交叉，能够生成新的表示
- 类别特征稀疏，罕见特征如何可能训练不充分——FTRL优化算法，自适应地调节学习率，常见的特征受训机会多，则步长减少一些；罕见特征受训机会少，则步长调整大一点。

# 3. emb

文章先引入了 LR 这个模型，特点在于强于记忆，能够记得历史上发生过的所有特征和组合。
- 所有模式，都依赖于人工的输入。
- 不能发掘出新模式，只能评估各个模式的重要性
- 强于记忆，弱于扩展

推荐算法不能仅满足于常见的模式（高频、常见），而是应该挖掘出来训练数据内罕见、低频、长尾的模式，才可以提升业务效果。

如果罕见的模式，去依靠人工去进行特征组合挖掘，必定会有局限，并且成本较大。

因此，有emb的概念引入。

## 3.1 共享还是独占emb

emb 实现较为简单，是随机化矩阵，每一行对应一个类别特征，然后用 SGD 随着模型一同优化。区别在于共享还是独占。

- 共享emb：能够缓解特征稀疏、训练数据不足的情况；另外是节省存储空间。比如FM算法，每个特征跟其他特征产生交叉时，运用的是唯一的emb。
- 独占emb：在不同业务领域的交互里，将同一个物料的emb进行隔离。比如下载和卸载同一个app的物料emb用不同的表示。

2021年，阿里的 Co-Action Network （CAN）利用不同的weight进行特征的交叉。

## 3.2 parameter server

简述：PS 是一个key-value的数据库形式，应对于 Master/Slave架构，大规模分布式训练时的困难。特征、emb维度非常高（可能到亿级别），无法在一个单机上装在，而是分布在集群里。

- worker：多个worker节点，会从server拉取（pull）模型参数，进行数据训练，给server推送（push）梯度
- server：多个server节点，每个节点只负责处理其中的一部分（shard）；面对pull 请求，会将最新的参数值发送回去；面对 push 请求，会聚合各个worker发送过来的梯度，通过SGD算法（Adam、Adagrad）更新模型参数
- scheduler：负责整个PS集群的管理，比如接受新节点的注册，将pull请求路由到合适的server节点


