---
title: "深度学习-细节"
date: 2023-03-01
toc: true
categories:
  - 深度学习-细节
classes: wide
words_per_minute: 10
---

### Batch Normalization

- 背景：机器学习中我们假设模型的输入数据是稳定分布的，但突破假设时称之为协变量偏移(covariance shift)。针对深度学习，经过$i-1$层后，进入 $i$层的数据分布可能发生变化，即"内容协变量偏移"。网络越深，该现象越明显。
- 导致的问题
  - 学习稳定性：网络每一层需要去适应输入分布的变化，影响学习效率，收敛过程可能不稳定
  - 网络参数更新：前几层参数的更新，可能使得后几层输入数据过大或者过小，进入到激活函数的饱和区，导致学习过程过早停止
- 在此前，一般用 非饱和的激活函数（ReLU），网络参数初始化，较低的学习率来避免这些问题
$$ y^{(k)} = \gamma^{(k)} \frac{x^{(k) - \mu^{(k)}}}{\sqrt{(\sigma^{(k)})^{2} + \sigma}} + \beta^{(k)}$$
- BN作用：
  - 在 $k$ 维度进行独立进行
  - 在mini-batch上学习输入数据的均值和标准差
  - 保留网络各层的学习成果 $\beta$，而不是退化为普通的标准化（不然输出的分布几乎不变，均值始终为0方差为1）
  - 如果后接激活函数，能够落到非饱和区域
  - 具备自我关闭能力，比如 $\beta$ 学习到原输入的均值，$\gamma$ 学习到原输入的标准差，则可以恢复初始的输入值
