---
title: "面试题回顾"
date: 2022-05-08
toc: true
categories:
  - interview
classes: wide
words_per_minute: 10
---

*以最简要（少字符）进行记录*

# NLP 常见问题

# CV 常见问题

# 树模型常见问题

### GBDT 原理

全程是 Gradient Boosting Decision Tree，是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，训练弱分类器，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。

每一轮迭代里，会计算当前模型在所有样本的负梯度，然后以该值为目标，训练一个新的弱分类器，并且拟合计算该弱分类器的权重，从而最小化 boosting 模型的损失。

### GBDT 的优点、局限性？

- 优点
  - 推理时，树可以并行化计算，速度快
  - 在分布稠密的数据集上效果、泛化能力均表现较好
  - 决策树作为弱分类器，让GBDT模型解释性较强，自动发现特征之间的*高阶关系*
- 局限性
  - 训练时需要串行训练
  - 在高维稀疏数据里，表现不如SVM或者DNN

### 梯度提升（GBDT）和梯度下降有什么区别和联系？

两者其实都是在每一步的迭代里，定义了损失函数，然后在损失函数相对于模型的负梯度的方向里，对模型进行更新。
- 梯度下降：一般就是对已定义的模型的参数进行更新
- 梯度提升：比如 boosting 的 GBDT 模型，是不断地叠加模型。模型是定义在函数空间里的，该函数空间影响到损失函数。因此，扩大了可以使用的模型种类（我理解是可以有完全不同的树结构，而不是同一个树结构不同的参数）。

### xgboost 介绍

xgboost的加法函数定义为：第t次迭代结果为 前t-1次迭代结果，加上第t棵树的结果

损失函数定义为 loss 加上 树整体的复杂度（叶子节点数目）

泰勒展开部分：

<figure>
  <img src="{{ '/assets/images/xgboost-img1.png' | relative_url }}" alt="xgboost"  class="center" style="max-height:600px; max-width:800px">
</figure>

> https://zhuanlan.zhihu.com/p/92837676

我们定义一颗树的复杂度 Ω，它由两部分组成： 叶子结点的数量； 叶子结点权重向量的L2范数；

### xgboost 和 GBDT 的差异

- 剪支、正则化
  - GBDT：基于经验损失的负梯度来构造新的决策树，到最后才剪枝
  - xgboost：在定义决策树构建阶段就剪枝，涉及到 叶子节点数限制、叶子节点的预测值。


- 如何并行化？

### xgboost 如何调参


